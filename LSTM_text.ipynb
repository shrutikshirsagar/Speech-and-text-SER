{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "from imblearn.datasets import make_imbalance\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(1792, 768)\n",
      "(553, 768)\n",
      "(264, 768)\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as spio\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "X_train, y_train, X_test, y_test, X_dev, y_dev= np.load('//home/amrgaballah/Desktop/a/anger_and_sad_only.npy', allow_pickle = True)\n",
    "print(X_train.dtype)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_dev.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "\n",
    "# min_max_scaler = preprocessing.StandardScaler()\n",
    "X_train = np.float32(X_train)\n",
    "X_test = np.float32(X_test)\n",
    "X_dev = np.float32(X_dev)\n",
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.47806764e-01  4.17328060e-01  9.99771893e-01 -9.93111432e-01\n",
      "  9.65131402e-01  8.38728249e-01  9.86115217e-01 -9.81538355e-01\n",
      " -9.62402821e-01 -6.91023946e-01  9.74688768e-01  9.98278379e-01\n",
      " -9.97152209e-01 -9.99737799e-01  7.43430912e-01 -9.78138268e-01\n",
      "  9.81952131e-01 -4.89345670e-01 -9.99942720e-01 -8.53503883e-01\n",
      " -4.62788880e-01 -9.99802053e-01  2.90580809e-01  9.62150037e-01\n",
      "  9.75405812e-01 -2.17174366e-02  9.82745051e-01  9.99952972e-01\n",
      "  8.31544995e-01  5.58376536e-02  2.86491871e-01 -9.85395908e-01\n",
      "  7.52935290e-01 -9.98257101e-01  1.87289327e-01  1.50550008e-01\n",
      "  4.98135239e-01 -1.77381530e-01  8.09455991e-01 -9.51167941e-01\n",
      " -7.08367586e-01 -7.90426731e-01  6.89730167e-01 -4.77898329e-01\n",
      "  7.10985482e-01  2.81741798e-01  1.95091844e-01 -6.18837476e-02\n",
      " -8.79066810e-02  9.99736488e-01 -9.67896700e-01  9.99110162e-01\n",
      " -9.88707900e-01  9.96922851e-01  9.96133983e-01  1.46470293e-01\n",
      "  9.94752824e-01  1.55680940e-01 -9.97308016e-01  2.84832925e-01\n",
      "  9.51500893e-01  2.19786137e-01  9.13701475e-01 -1.95064068e-01\n",
      " -1.33524492e-01 -5.20586789e-01 -8.23568285e-01  2.29654178e-01\n",
      " -5.05802333e-01  3.51159096e-01  2.06357077e-01  3.91777247e-01\n",
      "  9.89436030e-01 -8.67158711e-01  2.17865203e-02 -8.44299376e-01\n",
      "  4.78844792e-01 -9.99856412e-01  9.22087193e-01  9.99942899e-01\n",
      "  5.38078964e-01 -9.99635935e-01  9.92930472e-01 -2.11345091e-01\n",
      " -6.06437624e-01  2.76602596e-01 -9.96903837e-01 -9.99342263e-01\n",
      "  1.71775222e-02 -3.31022054e-01  8.55985105e-01 -9.79636848e-01\n",
      "  3.97327065e-01 -9.09360766e-01  9.99970794e-01 -6.81846380e-01\n",
      " -6.73447698e-02  3.47900540e-01  8.96809816e-01 -2.52136439e-01\n",
      " -7.57827103e-01  8.38027000e-01  9.97932255e-01 -9.91498053e-01\n",
      "  9.97482836e-01  5.64817667e-01 -9.24506783e-01 -8.68450105e-01\n",
      "  5.42080402e-01  1.06010335e-02  9.79336977e-01 -9.81536388e-01\n",
      " -7.95665622e-01 -9.07296017e-02  7.81868756e-01 -8.79700959e-01\n",
      "  9.87452686e-01  8.86279285e-01 -2.26936445e-01  9.99964535e-01\n",
      " -1.28985658e-01  9.54443216e-01  9.98564780e-01  8.58572900e-01\n",
      " -6.23174250e-01 -1.48787394e-01 -5.41256368e-01  9.10384715e-01\n",
      " -4.30926234e-01 -3.01186174e-01  7.61898279e-01 -9.89426076e-01\n",
      " -9.95650411e-01  9.99233902e-01 -1.60670131e-01  9.99962330e-01\n",
      " -9.98794794e-01  9.91553605e-01 -9.99914765e-01 -6.99805975e-01\n",
      " -5.27973831e-01 -5.57244532e-02 -9.74541724e-01  1.29048258e-01\n",
      "  9.87548351e-01  3.91803775e-03 -8.27594101e-01 -5.59094548e-01\n",
      "  3.29440445e-01 -7.85854340e-01  6.90201879e-01  6.95544720e-01\n",
      " -9.41666305e-01  9.96901214e-01  9.94678617e-01  9.36811745e-01\n",
      "  9.77612555e-01  6.20655566e-02 -9.29471791e-01  8.66903603e-01\n",
      "  9.77575004e-01 -9.99434829e-01  7.70289600e-01 -9.89610970e-01\n",
      "  9.98881578e-01  9.67096031e-01  6.69909060e-01 -9.90367472e-01\n",
      "  9.99867499e-01 -2.20722467e-01 -2.62254119e-01 -1.95338443e-01\n",
      " -3.85367036e-01 -9.98512149e-01  3.42121691e-01  3.87305439e-01\n",
      "  6.66265309e-01  9.99283254e-01 -9.92764235e-01  9.99546468e-01\n",
      "  9.73648131e-01 -1.91786170e-01  7.16583431e-01  9.97808874e-01\n",
      " -9.96088862e-01 -9.77251828e-01 -9.79591787e-01  3.28974456e-01\n",
      "  4.89832729e-01  6.12821877e-01  1.02653980e-01  9.62690473e-01\n",
      "  9.96615708e-01  6.46925569e-01 -9.98253644e-01 -2.90900052e-01\n",
      "  9.78746176e-01 -2.61851072e-01  9.99956131e-01 -1.38649523e-01\n",
      " -9.99670744e-01 -6.49462283e-01  9.02645886e-01  9.95593846e-01\n",
      " -2.54918993e-01  9.83932495e-01 -3.37588340e-01 -1.52068257e-01\n",
      "  9.71161723e-01 -9.98123169e-01  9.95984495e-01 -1.44535884e-01\n",
      "  6.54735744e-01  8.68359566e-01  9.88532484e-01 -8.37971509e-01\n",
      "  1.53742731e-04  3.09858590e-01 -6.45833015e-01  9.99827683e-01\n",
      " -9.99335349e-01 -9.72816125e-02  6.26500905e-01 -9.92774785e-01\n",
      " -9.97801423e-01  9.72803473e-01  5.36826104e-02 -5.37276924e-01\n",
      " -1.70354158e-01 -4.60070930e-03  2.27535427e-01  8.39670479e-01\n",
      "  9.90108252e-01 -5.11608899e-01 -4.18065846e-01 -9.99724746e-01\n",
      " -9.92243350e-01 -8.29642355e-01 -9.48110521e-01 -2.80342456e-02\n",
      "  6.26085877e-01 -3.97544771e-01 -8.84308279e-01 -9.96216595e-01\n",
      "  9.60479081e-01  7.32103825e-01 -8.16226125e-01 -5.40693700e-01\n",
      " -4.46704298e-01 -9.97190595e-01  6.04296088e-01 -8.57247412e-01\n",
      " -9.97222126e-01  9.99403715e-01 -5.97969234e-01  9.93756235e-01\n",
      "  9.80018198e-01 -9.90727425e-01  6.45076990e-01 -9.97961462e-01\n",
      " -8.69171396e-02 -9.97772276e-01  3.47794205e-01  2.38169760e-01\n",
      " -6.43373668e-01 -6.16217144e-02  9.94219840e-01 -9.47984815e-01\n",
      " -4.74791050e-01  7.00929999e-01 -9.99872148e-01  9.50326860e-01\n",
      " -2.35656649e-01  9.98975873e-01  8.59895647e-01  3.15124542e-01\n",
      "  9.82472479e-01  8.71682703e-01 -9.88566637e-01 -9.99705195e-01\n",
      "  8.39095771e-01  9.83675361e-01 -9.92195606e-01 -2.53201336e-01\n",
      "  9.99928772e-01 -9.95946884e-01 -7.04873264e-01 -9.39692378e-01\n",
      " -9.92929757e-01 -9.99458015e-01  9.84933972e-02 -7.98038721e-01\n",
      "  1.84043154e-01  9.82156873e-01 -9.25817862e-02  1.98314101e-01\n",
      "  9.94673491e-01  9.94896054e-01  2.94008851e-01 -5.79491854e-02\n",
      "  5.34773022e-02 -9.76164758e-01 -9.93215978e-01  4.56362456e-01\n",
      "  2.07564011e-01 -9.99947131e-01  9.99786615e-01 -9.94221926e-01\n",
      "  9.98473883e-01  7.58745015e-01 -9.90698397e-01  8.59835505e-01\n",
      "  1.20410033e-01 -9.51877594e-01  4.40375358e-02  9.99861956e-01\n",
      "  9.76904273e-01 -2.17322465e-02  5.51470853e-02  8.34240198e-01\n",
      " -1.76512793e-01  3.31349730e-01 -6.80251062e-01 -4.91456449e-01\n",
      "  1.85459808e-01 -9.30110633e-01  9.86315727e-01  4.00129795e-01\n",
      " -9.91761982e-01  9.95440543e-01  7.55562782e-02  6.77794278e-01\n",
      " -8.12371850e-01  7.91346908e-01  9.84326065e-01 -1.09455111e-02\n",
      " -2.76738465e-01 -2.02855676e-01 -7.79486716e-01 -9.69965935e-01\n",
      "  1.27751827e-01 -9.96391356e-01 -2.76038438e-01  9.33127344e-01\n",
      "  9.86672580e-01 -9.92923558e-01  9.94541764e-01 -9.38489139e-02\n",
      "  8.75119388e-01 -9.95472074e-01  9.99976754e-01 -9.98338342e-01\n",
      "  1.81334943e-01  5.82286358e-01 -7.92781651e-01 -8.75180811e-02\n",
      "  9.90222454e-01  9.69471037e-01  9.59634721e-01 -9.73060310e-01\n",
      " -6.96153879e-01  7.83522069e-01  9.75581050e-01 -9.75207806e-01\n",
      "  2.69715004e-02 -9.97908115e-01 -2.66022712e-01  9.93561566e-01\n",
      "  9.90220666e-01  2.40538642e-02 -5.39738774e-01 -9.95294034e-01\n",
      "  9.57572997e-01 -8.64363134e-01 -9.28132772e-01 -1.21495537e-01\n",
      " -7.37670481e-01  6.13438249e-01  9.96923745e-01 -3.95856231e-01\n",
      "  7.42431939e-01  1.45928338e-01 -9.84399915e-01  8.63309681e-01\n",
      "  7.00302362e-01  9.99817669e-01 -9.72544789e-01  2.37439856e-01\n",
      "  9.83275831e-01 -1.93958834e-01 -6.02187455e-01  6.30963802e-01\n",
      "  9.98221099e-01 -9.68147695e-01 -2.31743082e-01 -9.99402821e-01\n",
      " -3.36330459e-02 -7.41626978e-01  1.47694394e-01 -1.47656947e-01\n",
      "  2.71327607e-02 -5.55363357e-01  9.37977016e-01  2.16336817e-01\n",
      "  7.49303699e-01 -2.63405263e-01  9.51877177e-01 -3.51130813e-02\n",
      " -6.81976005e-02 -2.94790268e-01  8.08238238e-03  4.49157983e-01\n",
      "  3.01255345e-01  9.76926267e-01 -9.68119562e-01  9.99666750e-01\n",
      " -5.94723642e-01 -9.99935031e-01 -9.94360447e-01 -6.08342826e-01\n",
      " -9.99459386e-01  5.17609835e-01 -9.94677246e-01  9.87111866e-01\n",
      "  8.89336526e-01 -9.97068763e-01 -9.97936785e-01 -9.98044491e-01\n",
      " -9.97650146e-01  8.54354799e-01  3.74480426e-01  5.65808304e-02\n",
      " -2.70189822e-01  7.03797162e-01 -1.99619103e-02  9.68131423e-02\n",
      "  1.06875021e-02 -9.26995277e-01 -2.07413539e-01 -9.96871531e-01\n",
      "  4.39519882e-01 -9.99934018e-01 -5.78079283e-01  9.93130445e-01\n",
      " -9.91878748e-01 -9.20842230e-01 -9.27631795e-01 -6.40320837e-01\n",
      " -7.90034473e-01  4.97593194e-01  9.81743395e-01  1.53098524e-01\n",
      " -3.38751644e-01 -9.99241650e-01  9.87248123e-01 -6.06636226e-01\n",
      "  4.44162786e-02 -8.07053924e-01 -9.63570714e-01  9.99591172e-01\n",
      "  5.26954651e-01 -1.25633389e-01 -4.19839509e-02 -9.98750150e-01\n",
      "  9.82842863e-01 -9.05380666e-01 -9.16313052e-01 -9.78766620e-01\n",
      "  2.18052268e-01 -9.01641905e-01 -9.99768913e-01 -3.63342315e-02\n",
      "  9.94859576e-01  9.94522214e-01  9.65643585e-01  3.63698006e-01\n",
      " -3.85283113e-01 -9.32124436e-01  1.55447096e-01 -9.99933481e-01\n",
      "  5.56706309e-01  7.13934481e-01 -9.76883948e-01 -3.73505682e-01\n",
      "  9.90959764e-01  9.73322213e-01 -9.15142894e-01 -9.85227644e-01\n",
      "  6.22795701e-01  6.51350260e-01  9.26770926e-01 -1.30544528e-01\n",
      " -4.15553093e-01  3.10799301e-01 -5.03975572e-03 -9.83771741e-01\n",
      " -9.24279809e-01  9.96730626e-01 -9.98195469e-01  9.63719368e-01\n",
      "  9.90536034e-01  9.97877717e-01 -1.13416769e-01  1.46316931e-01\n",
      " -9.82998431e-01 -9.93134916e-01 -6.48744285e-01  3.31091106e-01\n",
      " -9.99898612e-01  9.99852717e-01 -9.99964952e-01  2.77640939e-01\n",
      " -4.42107052e-01  8.01912725e-01  9.87105787e-01 -3.93317223e-01\n",
      " -9.99872386e-01 -9.99705136e-01 -2.03094482e-01 -6.22965582e-02\n",
      "  9.87584293e-01  2.77035773e-01  1.77523226e-01 -4.91750330e-01\n",
      "  1.48691922e-01  9.97614324e-01 -7.94143677e-01 -4.83092457e-01\n",
      " -9.95496392e-01  9.99469638e-01  5.51901817e-01 -9.98221993e-01\n",
      "  9.93498683e-01 -9.99385536e-01  7.50495195e-01  9.64913130e-01\n",
      "  8.79939973e-01  9.74943638e-01 -9.98126090e-01  9.99957621e-01\n",
      " -9.99731243e-01  9.96673226e-01 -9.99971390e-01 -9.97753978e-01\n",
      "  9.99730825e-01 -9.81763542e-01 -6.84531212e-01 -9.99631286e-01\n",
      " -9.96916473e-01  2.64908254e-01  4.54993322e-02 -4.98930395e-01\n",
      "  9.87402916e-01 -9.99671102e-01 -9.98389184e-01  3.16656649e-01\n",
      " -8.97775412e-01 -6.07633531e-01  9.91714537e-01 -3.87911379e-01\n",
      "  9.89517689e-01 -1.63335457e-01  9.45015728e-01  1.42991170e-01\n",
      "  9.94188786e-01  9.97944236e-01 -8.03110659e-01 -6.90457880e-01\n",
      " -9.93334830e-01  9.68750954e-01 -4.93582040e-01  3.62860650e-01\n",
      "  9.43260550e-01 -2.13774703e-02 -4.53096062e-01  4.44199890e-01\n",
      " -9.97181654e-01  3.70150864e-01 -6.87819123e-01  7.38912284e-01\n",
      "  8.99176717e-01  8.19805443e-01 -5.38947210e-02 -5.22545934e-01\n",
      " -3.14901859e-01 -9.89792883e-01  5.90043366e-01 -9.99258220e-01\n",
      "  9.72517729e-01 -9.57861245e-01  1.13308705e-01 -3.80403101e-01\n",
      "  5.25546491e-01 -9.44280624e-01  9.99528050e-01  9.98517275e-01\n",
      " -9.98733580e-01  1.11453123e-01  9.87892509e-01 -3.55429441e-01\n",
      "  9.72254872e-01 -9.91527498e-01  4.14904505e-02  9.49077129e-01\n",
      " -7.24188209e-01  9.78580713e-01  4.19545799e-01 -3.99797969e-02\n",
      "  9.72350955e-01 -9.94854629e-01 -8.41574788e-01 -6.82766438e-01\n",
      "  3.83078933e-01  2.40853980e-01 -9.72369730e-01  8.07991177e-02\n",
      "  9.74669516e-01 -3.66715252e-01 -9.99678969e-01  8.98773015e-01\n",
      " -9.99367356e-01 -1.54557303e-01  9.81434405e-01  8.31297711e-02\n",
      "  9.99871969e-01 -6.82900846e-01  7.26398379e-02  2.16302902e-01\n",
      " -9.99711037e-01 -9.97973740e-01  9.61053222e-02 -1.47359163e-01\n",
      " -8.68128955e-01  9.97979820e-01  4.11783233e-02  8.48016560e-01\n",
      " -9.99792814e-01  2.47548178e-01  9.96185601e-01  3.24642181e-01\n",
      "  8.13766241e-01 -6.04310930e-01 -9.14470494e-01 -9.43179846e-01\n",
      " -6.55177295e-01  6.55521406e-03  7.82732069e-01 -9.93106484e-01\n",
      " -6.87184811e-01 -8.06800425e-01  9.99937773e-01 -9.97944891e-01\n",
      " -9.07831430e-01 -9.82037723e-01  2.30811954e-01  6.93487465e-01\n",
      "  4.38440979e-01  7.79368430e-02 -8.27326775e-01  8.13578606e-01\n",
      " -8.98549438e-01  9.97010887e-01 -9.95831549e-01 -9.95312750e-01\n",
      "  9.99745846e-01  3.60011727e-01 -9.93091583e-01 -1.71590209e-01\n",
      " -2.93197244e-01  1.09022990e-01  1.50987804e-01  6.09100282e-01\n",
      " -8.71783316e-01 -1.70743421e-01 -9.97705936e-01  7.65951037e-01\n",
      " -7.64272451e-01 -9.85502899e-01 -5.22766292e-01 -2.88619906e-01\n",
      " -9.98606145e-01  9.92228150e-01  9.73110318e-01  9.99933839e-01\n",
      " -9.99735475e-01  6.95425451e-01  9.26459208e-02  9.98894095e-01\n",
      " -1.33106038e-02 -6.53101921e-01  9.22375619e-01  9.99588549e-01\n",
      " -4.56500500e-01  8.10735464e-01 -1.32487297e-01  1.73033495e-03\n",
      "  2.66750932e-01 -7.20243931e-01  9.97001112e-01 -9.43774104e-01\n",
      "  6.38686955e-01 -9.76465046e-01 -9.99830961e-01  9.99878407e-01\n",
      " -6.31154999e-02  9.89654481e-01  3.24719578e-01  6.07548118e-01\n",
      " -8.14725697e-01  9.63727891e-01 -9.75871146e-01 -8.03068221e-01\n",
      " -9.99959767e-01  3.73372257e-01 -9.99086022e-01 -9.90403950e-01\n",
      "  3.89828570e-02  9.87791002e-01 -9.99438941e-01 -9.87910330e-01\n",
      " -2.83750296e-01 -9.99968827e-01  9.54403162e-01 -9.87356722e-01\n",
      " -6.40851617e-01 -9.82978046e-01  9.95494485e-01 -1.99296519e-01\n",
      " -4.57468271e-01  9.69601810e-01 -9.63760316e-01  9.31824923e-01\n",
      "  8.99088562e-01  5.54872394e-01  2.20790163e-01  1.44706398e-01\n",
      " -6.03900075e-01 -9.92505133e-01 -8.74918759e-01 -9.48510647e-01\n",
      "  7.88650513e-01 -9.92168427e-01 -7.81993926e-01  9.94248271e-01\n",
      "  9.83928382e-01 -9.98639047e-01 -9.95895982e-01  9.94216800e-01\n",
      " -3.89547914e-01  9.88428712e-01 -4.59081858e-01 -9.99806762e-01\n",
      " -9.99857306e-01  1.36961818e-01 -2.18052313e-01  9.92303371e-01\n",
      " -3.98469120e-01  9.95830059e-01  5.64507663e-01  7.20865652e-02\n",
      "  3.61862421e-01 -1.60824060e-01 -2.31854856e-01 -3.95565182e-01\n",
      " -1.67022541e-01  9.99946415e-01 -6.90433443e-01  9.90312994e-01]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Counter(y_train))\n",
    "# X_res, y_res = make_imbalance(X_train, y_train, sampling_strategy={0: 10, 1: 20, 2: 30}, random_state=42)\n",
    "# print(Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "# y_dev = keras.utils.to_categorical(y_dev, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "(20, 768)\n",
      "768\n",
      "20\n",
      "(1, 20, 768)\n",
      "1792\n",
      "(1792, 20, 768)\n"
     ]
    }
   ],
   "source": [
    "timestep = 20\n",
    "\n",
    "tmp_train = np.reshape(X_train[0], (1, len(X_train[0])))\n",
    "print(tmp_train.shape)\n",
    "tmp_train = np.repeat(tmp_train, timestep, axis=0)\n",
    "print(tmp_train.shape)\n",
    "print(len(tmp_train[0]))\n",
    "print(len(tmp_train[:,:]))\n",
    "ab = np.reshape(tmp_train, (1, len(tmp_train[:,:]), len(tmp_train[0])))\n",
    "print(ab.shape)\n",
    "print(X_train.shape[0])\n",
    "X_train = np.repeat( ab, X_train.shape[0], axis=0)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "(20, 768)\n",
      "768\n",
      "20\n",
      "(1, 20, 768)\n",
      "(553, 20, 768)\n"
     ]
    }
   ],
   "source": [
    "timestep = 20\n",
    "\n",
    "tmp_test = np.reshape(X_test[0], (1, len(X_test[0])))\n",
    "print(tmp_test.shape)\n",
    "tmp_test = np.repeat(tmp_test, timestep, axis=0)\n",
    "print(tmp_test.shape)\n",
    "print(len(tmp_test[0]))\n",
    "print(len(tmp_test[:,:]))\n",
    "abc = np.reshape(tmp_test, (1, len(tmp_test[:,:]), len(tmp_test[0])))\n",
    "print(abc.shape)\n",
    "X_test = np.repeat( abc, X_test.shape[0], axis=0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "(20, 768)\n",
      "768\n",
      "20\n",
      "(1, 20, 768)\n",
      "(264, 20, 768)\n"
     ]
    }
   ],
   "source": [
    "timestep = 20\n",
    "\n",
    "tmp_dev = np.reshape(X_dev[0], (1, len(X_dev[0])))\n",
    "print(tmp_dev.shape)\n",
    "tmp_dev = np.repeat(tmp_dev, timestep, axis=0)\n",
    "print(tmp_dev.shape)\n",
    "print(len(tmp_dev[0]))\n",
    "print(len(tmp_dev[:,:]))\n",
    "abc = np.reshape(tmp_dev, (1, len(tmp_dev[:,:]), len(tmp_dev[0])))\n",
    "print(abc.shape)\n",
    "X_dev = np.repeat( abc, X_dev.shape[0], axis=0)\n",
    "print(X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LSTM, Bidirectional, Input\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amrgaballah/anaconda3/envs/tensorflow_J1_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 20, 64)            213248    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 225,730\n",
      "Trainable params: 225,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "timestep = 20\n",
    "\n",
    "model = k.Sequential()\n",
    "model.add(LSTM(64, input_shape=(timestep, 768), return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(2))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rms = RMSprop(lr=0.01, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "Adam1 = Adam(lr=0.01)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam1,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "WARNING:tensorflow:From /home/amrgaballah/anaconda3/envs/tensorflow_J1_env/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1792 samples, validate on 264 samples\n",
      "Epoch 1/100\n",
      "1792/1792 [==============================] - 3s 2ms/sample - loss: 0.7387 - acc: 0.5619 - val_loss: 0.6810 - val_acc: 0.5795\n",
      "Epoch 2/100\n",
      "1792/1792 [==============================] - 1s 458us/sample - loss: 0.6731 - acc: 0.6172 - val_loss: 0.6903 - val_acc: 0.5795\n",
      "Epoch 3/100\n",
      "1792/1792 [==============================] - 1s 612us/sample - loss: 0.6669 - acc: 0.6183 - val_loss: 0.6807 - val_acc: 0.5795\n",
      "Epoch 4/100\n",
      "1792/1792 [==============================] - 1s 452us/sample - loss: 0.6674 - acc: 0.6186 - val_loss: 0.6860 - val_acc: 0.5795\n",
      "Epoch 5/100\n",
      "1792/1792 [==============================] - 1s 509us/sample - loss: 0.6667 - acc: 0.6189 - val_loss: 0.6815 - val_acc: 0.5795\n",
      "Epoch 6/100\n",
      "1792/1792 [==============================] - 1s 442us/sample - loss: 0.6668 - acc: 0.6189 - val_loss: 0.6840 - val_acc: 0.5795\n",
      "Epoch 7/100\n",
      "1792/1792 [==============================] - 1s 520us/sample - loss: 0.6665 - acc: 0.6189 - val_loss: 0.6822 - val_acc: 0.5795\n",
      "Epoch 8/100\n",
      "1792/1792 [==============================] - 1s 606us/sample - loss: 0.6660 - acc: 0.6189 - val_loss: 0.6832 - val_acc: 0.5795\n",
      "Epoch 9/100\n",
      "1792/1792 [==============================] - 1s 498us/sample - loss: 0.6655 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 10/100\n",
      "1792/1792 [==============================] - 1s 503us/sample - loss: 0.6664 - acc: 0.6189 - val_loss: 0.6827 - val_acc: 0.5795\n",
      "Epoch 11/100\n",
      "1792/1792 [==============================] - 1s 446us/sample - loss: 0.6665 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 12/100\n",
      "1792/1792 [==============================] - 1s 367us/sample - loss: 0.6655 - acc: 0.6189 - val_loss: 0.6828 - val_acc: 0.5795\n",
      "Epoch 13/100\n",
      "1792/1792 [==============================] - 1s 550us/sample - loss: 0.6666 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 14/100\n",
      "1792/1792 [==============================] - 1s 500us/sample - loss: 0.6667 - acc: 0.6189 - val_loss: 0.6828 - val_acc: 0.5795\n",
      "Epoch 15/100\n",
      "1792/1792 [==============================] - 1s 495us/sample - loss: 0.6655 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 16/100\n",
      "1792/1792 [==============================] - 1s 477us/sample - loss: 0.6662 - acc: 0.6189 - val_loss: 0.6826 - val_acc: 0.5795\n",
      "Epoch 17/100\n",
      "1792/1792 [==============================] - 1s 379us/sample - loss: 0.6664 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 18/100\n",
      "1792/1792 [==============================] - 1s 562us/sample - loss: 0.6663 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 19/100\n",
      "1792/1792 [==============================] - 1s 483us/sample - loss: 0.6654 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 20/100\n",
      "1792/1792 [==============================] - 1s 406us/sample - loss: 0.6666 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 21/100\n",
      "1792/1792 [==============================] - 1s 636us/sample - loss: 0.6667 - acc: 0.6189 - val_loss: 0.6827 - val_acc: 0.5795\n",
      "Epoch 22/100\n",
      "1792/1792 [==============================] - 1s 387us/sample - loss: 0.6665 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 23/100\n",
      "1792/1792 [==============================] - 1s 459us/sample - loss: 0.6666 - acc: 0.6189 - val_loss: 0.6826 - val_acc: 0.5795\n",
      "Epoch 24/100\n",
      "1792/1792 [==============================] - 1s 551us/sample - loss: 0.6662 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 25/100\n",
      "1792/1792 [==============================] - 1s 381us/sample - loss: 0.6661 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 26/100\n",
      "1792/1792 [==============================] - 1s 539us/sample - loss: 0.6660 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 27/100\n",
      "1792/1792 [==============================] - 1s 418us/sample - loss: 0.6657 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 28/100\n",
      "1792/1792 [==============================] - 1s 444us/sample - loss: 0.6661 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 29/100\n",
      "1792/1792 [==============================] - 1s 587us/sample - loss: 0.6666 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 30/100\n",
      "1792/1792 [==============================] - 1s 450us/sample - loss: 0.6661 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 31/100\n",
      "1792/1792 [==============================] - 1s 458us/sample - loss: 0.6659 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 32/100\n",
      "1792/1792 [==============================] - 1s 450us/sample - loss: 0.6658 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 33/100\n",
      "1792/1792 [==============================] - 1s 428us/sample - loss: 0.6659 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 34/100\n",
      "1792/1792 [==============================] - 1s 532us/sample - loss: 0.6664 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 35/100\n",
      "1792/1792 [==============================] - 1s 478us/sample - loss: 0.6657 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 36/100\n",
      "1792/1792 [==============================] - 1s 491us/sample - loss: 0.6656 - acc: 0.6189 - val_loss: 0.6822 - val_acc: 0.5795\n",
      "Epoch 37/100\n",
      "1792/1792 [==============================] - 1s 470us/sample - loss: 0.6659 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 38/100\n",
      "1792/1792 [==============================] - 1s 412us/sample - loss: 0.6663 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 39/100\n",
      "1792/1792 [==============================] - 1s 524us/sample - loss: 0.6663 - acc: 0.6189 - val_loss: 0.6822 - val_acc: 0.5795\n",
      "Epoch 40/100\n",
      "1792/1792 [==============================] - 1s 475us/sample - loss: 0.6666 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 41/100\n",
      "1792/1792 [==============================] - 1s 406us/sample - loss: 0.6662 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 42/100\n",
      "1792/1792 [==============================] - 1s 557us/sample - loss: 0.6657 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 43/100\n",
      "1792/1792 [==============================] - 1s 380us/sample - loss: 0.6657 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 44/100\n",
      "1792/1792 [==============================] - 1s 535us/sample - loss: 0.6661 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 45/100\n",
      "1792/1792 [==============================] - 1s 508us/sample - loss: 0.6663 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 46/100\n",
      "1792/1792 [==============================] - 1s 420us/sample - loss: 0.6664 - acc: 0.6189 - val_loss: 0.6822 - val_acc: 0.5795\n",
      "Epoch 47/100\n",
      "1792/1792 [==============================] - 1s 574us/sample - loss: 0.6659 - acc: 0.6189 - val_loss: 0.6825 - val_acc: 0.5795\n",
      "Epoch 48/100\n",
      "1792/1792 [==============================] - 1s 403us/sample - loss: 0.6660 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 49/100\n",
      "1792/1792 [==============================] - 1s 535us/sample - loss: 0.6664 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 50/100\n",
      "1792/1792 [==============================] - 1s 509us/sample - loss: 0.6667 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 51/100\n",
      "1792/1792 [==============================] - 0s 270us/sample - loss: 0.6662 - acc: 0.6189 - val_loss: 0.6821 - val_acc: 0.5795\n",
      "Epoch 52/100\n",
      "1792/1792 [==============================] - 1s 390us/sample - loss: 0.6660 - acc: 0.6189 - val_loss: 0.6824 - val_acc: 0.5795\n",
      "Epoch 53/100\n",
      "1792/1792 [==============================] - 1s 482us/sample - loss: 0.6662 - acc: 0.6189 - val_loss: 0.6823 - val_acc: 0.5795\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "hist = model.fit(X_train, y_train, validation_data = (X_dev, y_dev), verbose=1, nb_epoch=100, batch_size=256, shuffle=False, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6625610619082805\n",
      "Test accuracy: 0.6238698\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry     0.6239    1.0000    0.7684       345\n",
      "         Sad     0.0000    0.0000    0.0000       208\n",
      "\n",
      "   micro avg     0.6239    0.6239    0.6239       553\n",
      "   macro avg     0.3119    0.5000    0.3842       553\n",
      "weighted avg     0.3892    0.6239    0.4794       553\n",
      " samples avg     0.6239    0.6239    0.6239       553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "class_names = ['Angry', 'Sad']\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)\n",
    "accuracy_score(y_test, y_pred, normalize=False)\n",
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#Get the confusion matrix\n",
    "\n",
    "\n",
    "#array([[1, 0, 0],\n",
    "#   [1, 0, 0],\n",
    "#   [0, 1, 2]])\n",
    "cm =confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "#Now the normalize the diagonal entries\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#array([[1.        , 0.        , 0.        ],\n",
    "#      [1.        , 0.        , 0.        ],\n",
    "#      [0.        , 0.33333333, 0.66666667]])\n",
    "\n",
    "#The diagonal entries are the accuracies of each class\n",
    "cm.diagonal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
